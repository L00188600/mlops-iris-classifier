# .github/workflows/ct-cm.yml

name: Continuous Training and Monitoring

on:
  schedule:
    # Runs every Monday at 00:00 UTC (adjust as needed)
    - cron: '0 0 * * 1'
  workflow_dispatch: # Allows manual triggering

jobs:
  train-and-update-model:
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install google-cloud-storage # Ensure this is installed for GCS operations

      - name: Authenticate to Google Cloud (for GCS access)
        id: 'auth'
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
          project_id: '${{ secrets.GCP_PROJECT_ID }}'

      - name: Download training data from GCS (if applicable)
        run: |
          # Example: gsutil cp gs://your-gcs-bucket/data.csv data.csv
          # For Iris, we'll use the built-in dataset for simplicity,
          # but in a real project, you'd fetch your latest training data.
          echo "Using built-in Iris dataset for training."

      - name: Train model
        run: |
          python scripts/train.py

      - name: Upload new model to GCS
        env:
          PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BUCKET_NAME: mlops-iris-model-bucket-${{ secrets.GCP_PROJECT_ID }} # Dynamic bucket name
        run: |
          # Create bucket if it doesn't exist (only if it's the first time)
          # This command ensures the bucket exists before attempting upload.
          # The '|| true' allows the command to fail if bucket exists
          # without failing the step.
          gsutil mb -p "${{ env.PROJECT_ID }}" \
            "gs://${{ env.BUCKET_NAME }}" || true

          # Upload the trained model
          gsutil cp models/iris_logistic_regression_model.joblib \
            "gs://${{ env.BUCKET_NAME }}/iris_model/iris_logistic_regression_model.joblib"

  monitor-deployment:
    needs: train-and-update-model # This job depends on model training
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install Flask requests # Install Flask and requests for API testing

      - name: Monitor Deployed API
        env:
          VM_INSTANCE_NAME: ${{ secrets.VM_INSTANCE_NAME }}
          GCP_ZONE: ${{ secrets.GCP_ZONE }}
          PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        run: |
          # Get VM's external IP address
          VM_IP=$(gcloud compute instances describe "${{ env.VM_INSTANCE_NAME }}" \
                  --zone="${{ env.GCP_ZONE }}" \
                  --format='get(networkInterfaces[0].accessConfigs[0].natIP)' \
                  --project="${{ env.PROJECT_ID }}")

          if [ -z "$VM_IP" ]; then
            echo "Error: Could not retrieve VM IP address."
            exit 1
          fi

          echo "VM External IP: $VM_IP"
          API_URL="http://${VM_IP}:5000/predict"
          HOME_URL="http://${VM_IP}:5000/"

          echo "Testing home endpoint..."
          curl -f --retry 5 --retry-delay 10 "${HOME_URL}"

          echo "Testing predict endpoint (after 30 seconds for app to fully " \
               "start)..." # Split for 78-char limit
          sleep 30 # Give the Flask app some time to load the model after deployment

          # Example test data
          TEST_DATA='{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'

          # Send a POST request to the predict endpoint
          response=$(curl -s -X POST -H "Content-Type: application/json" \
                           -d "${TEST_DATA}" "${API_URL}")

          echo "API Response: ${response}"

          # Basic check for a successful response (e.g., contains 'prediction')
          if echo "${response}" | grep -q "prediction"; then
            echo "API monitoring successful!"
          else
            echo "API monitoring failed: Unexpected response."
            exit 1
          fi
